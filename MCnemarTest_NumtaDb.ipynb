{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-H-Sumon/FusionNet/blob/main/MCnemarTest_NumtaDb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "95686169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c773fe51-57e3-4f00-97f6-e7a781b2d8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.feature import hog\n",
        "from skimage.filters import gabor\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "from termcolor import colored\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "M2QQEdLR06ul"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-kr1iv78nqUM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d1ee80bb-535e-457b-b4b1-d37e045fa9d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e96eaa4-2c9d-4219-84ca-16bb2258efec\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e96eaa4-2c9d-4219-84ca-16bb2258efec\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"anowerhossensumon\",\"key\":\"157d4cd7ff6a9132eebc558a7d7fb011\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VPga7XY3ISRm"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bea446c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cab5d39-a11a-428d-add7-eb06cdca3fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/BengaliAI/numta\n",
            "License(s): CC-BY-SA-4.0\n",
            "Downloading numta.zip to /content\n",
            " 99% 1.89G/1.91G [00:20<00:00, 65.1MB/s]\n",
            "100% 1.91G/1.91G [00:20<00:00, 98.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d BengaliAI/numta --unzip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vdjbUnOLMlH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b34c26f-a50c-4cab-e7da-f294cb26b207"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tVMbNwAdnSSr"
      },
      "outputs": [],
      "source": [
        "# ======================== Data Loading ========================\n",
        "base_path = \"/content\"\n",
        "\n",
        "\n",
        "\n",
        "# Load and combine all labeled training data\n",
        "all_image_paths = []\n",
        "all_labels = []\n",
        "\n",
        "for file_name in os.listdir(base_path):\n",
        "    if file_name.startswith(\"training-\") and file_name.endswith(\".csv\"):\n",
        "        train_file_path = os.path.join(base_path, file_name)\n",
        "        labels_df = pd.read_csv(train_file_path)\n",
        "        folder_name = file_name.replace(\".csv\", \"\")\n",
        "        folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "        if os.path.exists(folder_path):\n",
        "            for _, row in labels_df.iterrows():\n",
        "                image_file = row['filename']\n",
        "                label = row['digit']\n",
        "                image_path = os.path.join(folder_path, image_file)\n",
        "                all_image_paths.append(image_path)\n",
        "                all_labels.append(label)\n",
        "\n",
        "# ===================== Train/Test Split =====================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    all_image_paths, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
        ")\n",
        "\n",
        "# ===================== Create DataFrames =====================\n",
        "train_df = pd.DataFrame({\"image_path\": X_train, \"label\": y_train})\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test_df_numtadb.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frWuQ6q11XVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fdba2a6-f087-4efd-b527-44537a62e642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8 9 5 7 6 0 3 1 2 4]\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "print(train_df['label'].unique())  # Check unique labels in the dataset\n",
        "print(train_df['label'].nunique())  # Check the number of unique classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4f8da4f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a3f54b-34bc-4e26-dc35-9beb314c87f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training images: 57636\n",
            "Total testing images: 14409\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total training images: {len(train_df)}\")\n",
        "print(f\"Total testing images: {len(test_df)}\")\n",
        "print(test_df['label'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1c163614"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import cv2.ximgproc as xip\n",
        "\n",
        "def preprocess_image(file_path):\n",
        "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return np.zeros((28, 28))  # Placeholder for missing images\n",
        "\n",
        "    # Apply Otsu's Thresholding\n",
        "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Resize image to 28x28\n",
        "    img = cv2.resize(img, (28, 28), interpolation=cv2.INTER_AREA) / 255.0\n",
        "\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M9io7DBwNbGK"
      },
      "outputs": [],
      "source": [
        "# ======================== Load & Preprocess Images ========================\n",
        "train_images = np.array([preprocess_image(fp) for fp in train_df['image_path'].values])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0c9f1f30"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from joblib import Parallel, delayed\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "def augment_image(img):\n",
        "    rows, cols = img.shape\n",
        "\n",
        "    # Random Rotation\n",
        "    angle = random.uniform(-15, 15)\n",
        "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    rotated = cv2.warpAffine(img, M, (cols, rows))\n",
        "\n",
        "    # Random Shifting\n",
        "    tx = random.uniform(-2, 2)\n",
        "    ty = random.uniform(-2, 2)\n",
        "    M_shift = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    shifted = cv2.warpAffine(rotated, M_shift, (cols, rows))\n",
        "\n",
        "    return shifted\n",
        "\n",
        "def augment_dataset(train_images):\n",
        "    \"\"\"Applies augmentation to a dataset without parallel processing.\"\"\"\n",
        "    augmented_images = []\n",
        "    for img in train_images:\n",
        "        augmented = augment_image(img)\n",
        "        augmented_images.append(augmented)\n",
        "    return np.array(augmented_images)\n",
        "\n",
        "# Apply augmentation to dataset\n",
        "augmented_images = augment_dataset(train_images)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EVdthSUnGwyg"
      },
      "outputs": [],
      "source": [
        "# cores = multiprocessing.cpu_count()\n",
        "# print(cores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r8zA8_UCPXMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e5e5ac-6265-4469-e538-2a8bebe7ce26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mahotas) (2.0.2)\n",
            "Downloading mahotas-1.4.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.18\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install mahotas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9060b8d6"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from skimage.feature import hog, local_binary_pattern\n",
        "from skimage.filters import gabor\n",
        "from mahotas.features import zernike_moments\n",
        "\n",
        "# Feature Extraction\n",
        "def extract_features_single(img):\n",
        "    # Extract HOG features\n",
        "    hog_features = hog(img, pixels_per_cell=(4, 4), cells_per_block=(2, 2), feature_vector=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Extract LBP features\n",
        "    lbp = local_binary_pattern(img, P=8, R=1, method=\"uniform\")\n",
        "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 10))\n",
        "    lbp_hist = lbp_hist.astype(\"float\")\n",
        "    lbp_hist /= lbp_hist.sum()  # Normalize LBP histogram\n",
        "\n",
        "    return np.hstack([hog_features, lbp_hist])\n",
        "\n",
        "def extract_features(images):\n",
        "    features = []\n",
        "    for img in images:\n",
        "        feat = extract_features_single(img)\n",
        "        features.append(feat)\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "# Apply feature extraction\n",
        "augmented_features = extract_features(augmented_images)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5c02952b"
      },
      "outputs": [],
      "source": [
        "# ======================== Data Preparation ========================\n",
        "X_train_preprocessed = np.array([preprocess_image(fp) for fp in train_df['image_path']])\n",
        "X_test_preprocessed = np.array([preprocess_image(fp) for fp in test_df['image_path']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nsq32NpNTfXP"
      },
      "outputs": [],
      "source": [
        "# Apply augmentation\n",
        "X_train_augmented = augment_dataset(X_train_preprocessed)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "exrR8qUkTfZ7"
      },
      "outputs": [],
      "source": [
        "# Extract features from augmented images\n",
        "X_train_features = extract_features(X_train_augmented)\n",
        "X_test_features = extract_features(X_test_preprocessed)  # No augmentation for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "77d5a6f4"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_features = scaler.fit_transform(X_train_features)\n",
        "X_test_features = scaler.transform(X_test_features)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(train_df['label'])\n",
        "y_test = encoder.fit_transform(test_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1fd3216e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc8de78-9b68-4477-f600-4493a61285e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GE7IHILLiyvx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "def load_images(image_paths):\n",
        "    data = []\n",
        "    for path in image_paths:\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
        "        img = cv2.resize(img, (64, 64))  # Resize to a fixed size\n",
        "        img = img.flatten()  # Convert to 1D array\n",
        "        data.append(img)\n",
        "    return np.array(data)\n",
        "\n",
        "# Assuming X_train originally contains image file paths\n",
        "X_train = load_images(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lv5AK_wjiamz"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.utils import resample\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "# ======= Step 1: Sample a Subset of the Training Data =======\n",
        "X_sampled, y_sampled = resample(\n",
        "    X_train_features, y_train, n_samples=10000, stratify=y_train, random_state=42\n",
        ")\n",
        "# ======= Step 2: Define the Objective Function =======\n",
        "def objective(trial):\n",
        "    k = trial.suggest_int('n_neighbors', 3, 15)\n",
        "    metric = trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])\n",
        "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
        "\n",
        "    model = KNeighborsClassifier(n_neighbors=k, metric=metric, weights=weights)\n",
        "\n",
        "    # Use cv=3 for faster tuning\n",
        "    score = cross_val_score(model, X_sampled, y_sampled, cv=3, scoring='accuracy').mean()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "whSJfi1oV0vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe48e7e0-559f-44f9-ea64-08d862450c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-25 08:30:52,274] A new study created in memory with name: no-name-eea3854c-78eb-49fb-82d1-7fd3345f64ac\n",
            "[I 2025-08-25 08:31:14,097] Trial 0 finished with value: 0.6728018932667114 and parameters: {'n_neighbors': 3, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 0 with value: 0.6728018932667114.\n",
            "[I 2025-08-25 08:31:15,915] Trial 1 finished with value: 0.6563006030657056 and parameters: {'n_neighbors': 14, 'metric': 'euclidean', 'weights': 'uniform'}. Best is trial 0 with value: 0.6728018932667114.\n",
            "[I 2025-08-25 08:31:34,839] Trial 2 finished with value: 0.6565005630737039 and parameters: {'n_neighbors': 11, 'metric': 'euclidean', 'weights': 'uniform'}. Best is trial 0 with value: 0.6728018932667114.\n",
            "[I 2025-08-25 08:31:36,073] Trial 3 finished with value: 0.6380011426457582 and parameters: {'n_neighbors': 5, 'metric': 'minkowski', 'weights': 'uniform'}. Best is trial 0 with value: 0.6728018932667114.\n",
            "[I 2025-08-25 08:31:45,596] Trial 5 finished with value: 0.6934012837396516 and parameters: {'n_neighbors': 6, 'metric': 'minkowski', 'weights': 'distance'}. Best is trial 5 with value: 0.6934012837396516.\n",
            "[I 2025-08-25 08:31:57,397] Trial 6 finished with value: 0.7035009739726222 and parameters: {'n_neighbors': 10, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 6 with value: 0.7035009739726222.\n",
            "[I 2025-08-25 08:34:22,603] Trial 4 finished with value: 0.6142011621678064 and parameters: {'n_neighbors': 4, 'metric': 'manhattan', 'weights': 'uniform'}. Best is trial 6 with value: 0.7035009739726222.\n",
            "[I 2025-08-25 08:34:32,110] Trial 8 finished with value: 0.6998013238636401 and parameters: {'n_neighbors': 8, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 6 with value: 0.7035009739726222.\n",
            "[I 2025-08-25 08:34:44,615] Trial 7 finished with value: 0.6364002627017351 and parameters: {'n_neighbors': 10, 'metric': 'manhattan', 'weights': 'uniform'}. Best is trial 6 with value: 0.7035009739726222.\n",
            "[I 2025-08-25 08:34:56,490] Trial 10 finished with value: 0.7035009739726222 and parameters: {'n_neighbors': 10, 'metric': 'minkowski', 'weights': 'distance'}. Best is trial 6 with value: 0.7035009739726222.\n",
            "[I 2025-08-25 08:35:08,224] Trial 11 finished with value: 0.7095010040896111 and parameters: {'n_neighbors': 14, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 11 with value: 0.7095010040896111.\n",
            "[I 2025-08-25 08:35:17,966] Trial 12 finished with value: 0.70650076405361 and parameters: {'n_neighbors': 15, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 11 with value: 0.7095010040896111.\n",
            "[I 2025-08-25 08:35:29,889] Trial 13 finished with value: 0.70650076405361 and parameters: {'n_neighbors': 15, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 11 with value: 0.7095010040896111.\n",
            "[I 2025-08-25 08:35:41,698] Trial 14 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:35:52,245] Trial 15 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:37:18,617] Trial 9 finished with value: 0.6946012237696474 and parameters: {'n_neighbors': 14, 'metric': 'manhattan', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:37:30,266] Trial 17 finished with value: 0.7072006540786052 and parameters: {'n_neighbors': 12, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:37:42,047] Trial 18 finished with value: 0.7072006540786052 and parameters: {'n_neighbors': 12, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:38:35,707] Trial 16 finished with value: 0.6953011737886445 and parameters: {'n_neighbors': 12, 'metric': 'manhattan', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:38:47,752] Trial 20 finished with value: 0.6998013238636401 and parameters: {'n_neighbors': 8, 'metric': 'minkowski', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:38:59,563] Trial 21 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:39:09,021] Trial 22 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:39:20,766] Trial 23 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:39:33,424] Trial 24 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:39:50,020] Trial 25 finished with value: 0.7085008740826093 and parameters: {'n_neighbors': 11, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:40:01,374] Trial 26 finished with value: 0.6998013238636401 and parameters: {'n_neighbors': 8, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:40:11,398] Trial 27 finished with value: 0.6564002531026948 and parameters: {'n_neighbors': 15, 'metric': 'euclidean', 'weights': 'uniform'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:40:23,228] Trial 28 finished with value: 0.7085008740826093 and parameters: {'n_neighbors': 11, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:40:35,128] Trial 29 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'minkowski', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:40:36,188] Trial 19 finished with value: 0.6953011737886445 and parameters: {'n_neighbors': 12, 'metric': 'manhattan', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:40:48,310] Trial 31 finished with value: 0.7095010040896111 and parameters: {'n_neighbors': 14, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:40:57,910] Trial 32 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:41:09,610] Trial 33 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:41:21,669] Trial 34 finished with value: 0.7095010040896111 and parameters: {'n_neighbors': 14, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:41:33,414] Trial 35 finished with value: 0.6565005630737039 and parameters: {'n_neighbors': 11, 'metric': 'euclidean', 'weights': 'uniform'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:41:43,118] Trial 36 finished with value: 0.6510009129287254 and parameters: {'n_neighbors': 9, 'metric': 'euclidean', 'weights': 'uniform'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:41:55,009] Trial 37 finished with value: 0.7072006540786052 and parameters: {'n_neighbors': 12, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:42:06,962] Trial 38 finished with value: 0.6728018932667114 and parameters: {'n_neighbors': 3, 'metric': 'minkowski', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:42:18,030] Trial 39 finished with value: 0.6377014126127669 and parameters: {'n_neighbors': 6, 'metric': 'euclidean', 'weights': 'uniform'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:43:21,998] Trial 30 finished with value: 0.6889007137066435 and parameters: {'n_neighbors': 9, 'metric': 'manhattan', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:43:33,326] Trial 41 finished with value: 0.6564002531026948 and parameters: {'n_neighbors': 15, 'metric': 'euclidean', 'weights': 'uniform'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:43:43,425] Trial 42 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:43:55,440] Trial 43 finished with value: 0.7110007641436008 and parameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:44:07,495] Trial 44 finished with value: 0.7095010040896111 and parameters: {'n_neighbors': 14, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:44:18,667] Trial 45 finished with value: 0.7035009739726222 and parameters: {'n_neighbors': 10, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:44:29,036] Trial 46 finished with value: 0.7072006540786052 and parameters: {'n_neighbors': 12, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:44:40,830] Trial 47 finished with value: 0.7095010040896111 and parameters: {'n_neighbors': 14, 'metric': 'minkowski', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:45:05,018] Trial 40 finished with value: 0.6920011137286494 and parameters: {'n_neighbors': 15, 'metric': 'manhattan', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:45:14,989] Trial 49 finished with value: 0.7085008740826093 and parameters: {'n_neighbors': 11, 'metric': 'euclidean', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n",
            "[I 2025-08-25 08:47:01,452] Trial 48 finished with value: 0.6945012737626491 and parameters: {'n_neighbors': 11, 'metric': 'manhattan', 'weights': 'distance'}. Best is trial 14 with value: 0.7110007641436008.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN hyperparameters: {'n_neighbors': 13, 'metric': 'euclidean', 'weights': 'distance'}\n"
          ]
        }
      ],
      "source": [
        "# ======= Step 3: Create Study with Sampler and Pruner =======\n",
        "study_knn = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=TPESampler(seed=42),\n",
        "    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n",
        ")\n",
        "\n",
        "# ======= Step 4: Optimize with Parallel Processing =======\n",
        "study_knn.optimize(objective, n_trials=50, n_jobs=2)  # Use 2 cores (Colab-safe)\n",
        "\n",
        "# ======= Step 5: Retrieve Best Parameters =======\n",
        "best_knn_params = study_knn.best_params\n",
        "print(\"Best KNN hyperparameters:\", best_knn_params)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "c0hY__1DV0x6"
      },
      "outputs": [],
      "source": [
        "# Extract best values\n",
        "best_k = best_knn_params['n_neighbors']\n",
        "best_metric = best_knn_params['metric']\n",
        "best_weights = best_knn_params['weights']\n",
        "\n",
        "def knn(X_train, y_train, X_test, k=best_k, metric=best_metric, weights=best_weights):\n",
        "    \"\"\"Optimized KNN model.\"\"\"\n",
        "\n",
        "    # Convert labels to NumPy array (avoid indexing issues)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    # Fit KNN classifier with optimized parameters\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k, metric=metric, weights=weights)\n",
        "    knn_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = knn_model.predict(X_test)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5e3ed9be"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Run KNN with optimized parameters\n",
        "knn_preds = knn(X_train_features, y_train, X_test_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7502eed8"
      },
      "outputs": [],
      "source": [
        "X_train_combined = np.hstack((X_train_features, y_train.reshape(-1, 1)))\n",
        "X_test_combined = np.hstack((X_test_features, knn_preds.reshape(-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IDTsEzYyUVA_"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "feature_dim = X_train_combined.shape[1]  # Assuming X_train is your feature matrix\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Objective function for optimizing neural network hyperparameters.\"\"\"\n",
        "\n",
        "    # Hyperparameters to optimize\n",
        "    num_units_1 = trial.suggest_int('num_units_1', 128, 512, step=64)  # First Dense layer\n",
        "    num_units_2 = trial.suggest_int('num_units_2', 64, 256, step=64)   # Second Dense layer\n",
        "    num_units_3 = trial.suggest_int('num_units_3', 32, 128, step=32)   # Third Dense layer\n",
        "    dropout_1 = trial.suggest_float('dropout_1', 0.2, 0.5)  # Dropout after first layer\n",
        "    dropout_2 = trial.suggest_float('dropout_2', 0.2, 0.5)  # Dropout after second layer\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 0.0001, 0.001)  # Adam learning rate\n",
        "\n",
        "    # Build model\n",
        "    model = Sequential([\n",
        "        Dense(num_units_1, activation='selu', input_shape=(feature_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_1),\n",
        "        Dense(num_units_2, activation='selu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_2),\n",
        "        Dense(num_units_3, activation='selu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model with tuned learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train model (Use fewer epochs for tuning)\n",
        "    history = model.fit(\n",
        "        X_train_combined, y_train,\n",
        "        epochs=10,  # Keep epochs low for tuning\n",
        "        batch_size=64,\n",
        "        validation_split=0.2,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Return best validation accuracy\n",
        "    return max(history.history['val_accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "e4631237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46be752a-fd1f-4766-e868-02b9bf203fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-25 08:48:25,105] A new study created in memory with name: no-name-df561f63-a311-4cb6-bb3f-c0e2a5762028\n",
            "/tmp/ipython-input-3243621785.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 0.0001, 0.001)  # Adam learning rate\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-08-25 08:51:08,191] Trial 0 finished with value: 0.9858605265617371 and parameters: {'num_units_1': 448, 'num_units_2': 128, 'num_units_3': 96, 'dropout_1': 0.3288322399958822, 'dropout_2': 0.3009081470817634, 'learning_rate': 0.0004915733874552764}. Best is trial 0 with value: 0.9858605265617371.\n",
            "[I 2025-08-25 08:54:13,057] Trial 1 finished with value: 0.983605146408081 and parameters: {'num_units_1': 448, 'num_units_2': 256, 'num_units_3': 96, 'dropout_1': 0.3829528315676978, 'dropout_2': 0.34974007099231513, 'learning_rate': 0.00023092278015701074}. Best is trial 0 with value: 0.9858605265617371.\n",
            "[I 2025-08-25 08:55:31,774] Trial 2 finished with value: 0.9799618124961853 and parameters: {'num_units_1': 128, 'num_units_2': 128, 'num_units_3': 128, 'dropout_1': 0.21775256404428647, 'dropout_2': 0.3249751671087966, 'learning_rate': 0.0001924295429532532}. Best is trial 0 with value: 0.9858605265617371.\n",
            "[I 2025-08-25 08:58:12,537] Trial 3 finished with value: 0.9857737421989441 and parameters: {'num_units_1': 384, 'num_units_2': 192, 'num_units_3': 128, 'dropout_1': 0.43719596192889476, 'dropout_2': 0.271360582138955, 'learning_rate': 0.0005047374138088716}. Best is trial 0 with value: 0.9858605265617371.\n",
            "[I 2025-08-25 08:59:38,116] Trial 4 finished with value: 0.978226900100708 and parameters: {'num_units_1': 128, 'num_units_2': 64, 'num_units_3': 128, 'dropout_1': 0.2589261734047017, 'dropout_2': 0.459274466698839, 'learning_rate': 0.0003001591227875936}. Best is trial 0 with value: 0.9858605265617371.\n",
            "[I 2025-08-25 09:01:11,907] Trial 5 finished with value: 0.9829111695289612 and parameters: {'num_units_1': 128, 'num_units_2': 128, 'num_units_3': 96, 'dropout_1': 0.3109976463420435, 'dropout_2': 0.3308170441164828, 'learning_rate': 0.0005660050866242522}. Best is trial 0 with value: 0.9858605265617371.\n",
            "[I 2025-08-25 09:02:38,930] Trial 6 finished with value: 0.9579285383224487 and parameters: {'num_units_1': 128, 'num_units_2': 256, 'num_units_3': 32, 'dropout_1': 0.2948767822441614, 'dropout_2': 0.37625850159228424, 'learning_rate': 0.00010670022161535484}. Best is trial 0 with value: 0.9858605265617371.\n",
            "[I 2025-08-25 09:04:17,761] Trial 7 finished with value: 0.9859472513198853 and parameters: {'num_units_1': 256, 'num_units_2': 256, 'num_units_3': 96, 'dropout_1': 0.488201147798374, 'dropout_2': 0.3696024309503174, 'learning_rate': 0.0007285252774061576}. Best is trial 7 with value: 0.9859472513198853.\n",
            "[I 2025-08-25 09:05:59,077] Trial 8 finished with value: 0.9751908183097839 and parameters: {'num_units_1': 320, 'num_units_2': 64, 'num_units_3': 64, 'dropout_1': 0.38918536368931667, 'dropout_2': 0.4753904473619988, 'learning_rate': 0.00019039365273884607}. Best is trial 7 with value: 0.9859472513198853.\n",
            "[I 2025-08-25 09:08:43,844] Trial 9 finished with value: 0.9840388894081116 and parameters: {'num_units_1': 448, 'num_units_2': 128, 'num_units_3': 64, 'dropout_1': 0.41531520765784535, 'dropout_2': 0.41320914344507575, 'learning_rate': 0.0004474629919288865}. Best is trial 7 with value: 0.9859472513198853.\n",
            "[I 2025-08-25 09:10:21,355] Trial 10 finished with value: 0.9861207604408264 and parameters: {'num_units_1': 256, 'num_units_2': 192, 'num_units_3': 32, 'dropout_1': 0.49944989056073197, 'dropout_2': 0.2168366230912907, 'learning_rate': 0.0009213480460994554}. Best is trial 10 with value: 0.9861207604408264.\n",
            "[I 2025-08-25 09:11:59,074] Trial 11 finished with value: 0.9866412281990051 and parameters: {'num_units_1': 256, 'num_units_2': 192, 'num_units_3': 32, 'dropout_1': 0.4786975841817194, 'dropout_2': 0.21657341879159725, 'learning_rate': 0.0009905464756923656}. Best is trial 11 with value: 0.9866412281990051.\n",
            "[I 2025-08-25 09:13:33,278] Trial 12 finished with value: 0.9854267835617065 and parameters: {'num_units_1': 256, 'num_units_2': 192, 'num_units_3': 32, 'dropout_1': 0.48739576709462323, 'dropout_2': 0.20353584770879993, 'learning_rate': 0.0009024086818450213}. Best is trial 11 with value: 0.9866412281990051.\n",
            "[I 2025-08-25 09:15:14,073] Trial 13 finished with value: 0.9865545034408569 and parameters: {'num_units_1': 256, 'num_units_2': 192, 'num_units_3': 32, 'dropout_1': 0.45532373330684117, 'dropout_2': 0.21093999549324738, 'learning_rate': 0.0009761811157965916}. Best is trial 11 with value: 0.9866412281990051.\n",
            "[I 2025-08-25 09:17:04,311] Trial 14 finished with value: 0.9868147373199463 and parameters: {'num_units_1': 320, 'num_units_2': 192, 'num_units_3': 64, 'dropout_1': 0.44875006368671555, 'dropout_2': 0.2471380987724786, 'learning_rate': 0.000995942290051766}. Best is trial 14 with value: 0.9868147373199463.\n",
            "[I 2025-08-25 09:18:55,846] Trial 15 finished with value: 0.9862074851989746 and parameters: {'num_units_1': 320, 'num_units_2': 192, 'num_units_3': 64, 'dropout_1': 0.37324739001065543, 'dropout_2': 0.2583278541684272, 'learning_rate': 0.0006969470343975543}. Best is trial 14 with value: 0.9868147373199463.\n",
            "[I 2025-08-25 09:20:45,575] Trial 16 finished with value: 0.9834316372871399 and parameters: {'num_units_1': 192, 'num_units_2': 256, 'num_units_3': 64, 'dropout_1': 0.44718946640342483, 'dropout_2': 0.2526511382348607, 'learning_rate': 0.0003567205281697286}. Best is trial 14 with value: 0.9868147373199463.\n",
            "[I 2025-08-25 09:23:51,075] Trial 17 finished with value: 0.9844725728034973 and parameters: {'num_units_1': 512, 'num_units_2': 192, 'num_units_3': 32, 'dropout_1': 0.4122845722554297, 'dropout_2': 0.286412532905898, 'learning_rate': 0.0006977870167147341}. Best is trial 14 with value: 0.9868147373199463.\n",
            "[I 2025-08-25 09:26:13,754] Trial 18 finished with value: 0.9725017547607422 and parameters: {'num_units_1': 384, 'num_units_2': 128, 'num_units_3': 64, 'dropout_1': 0.4667172228211315, 'dropout_2': 0.24055631661855223, 'learning_rate': 0.00010468964541993778}. Best is trial 14 with value: 0.9868147373199463.\n",
            "[I 2025-08-25 09:27:54,527] Trial 19 finished with value: 0.98273766040802 and parameters: {'num_units_1': 192, 'num_units_2': 256, 'num_units_3': 32, 'dropout_1': 0.3498068809776446, 'dropout_2': 0.3033697800526878, 'learning_rate': 0.0003752715039425445}. Best is trial 14 with value: 0.9868147373199463.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_units_1': 320, 'num_units_2': 192, 'num_units_3': 64, 'dropout_1': 0.44875006368671555, 'dropout_2': 0.2471380987724786, 'learning_rate': 0.000995942290051766}\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize')  # Maximize accuracy\n",
        "study.optimize(objective, n_trials=20)  # Run 20 trials\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = study.best_params\n",
        "print(\"Best hyperparameters:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DB7EgQnxVa6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cc7557-71fc-4886-8bfd-adb4949919fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Training Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\n",
            "===== Training Fold 2 =====\n",
            "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\n",
            "===== Training Fold 3 =====\n",
            "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
            "\n",
            "===== Training Fold 4 =====\n",
            "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\n",
            "===== Training Fold 5 =====\n",
            "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
          ]
        }
      ],
      "source": [
        "# Unpack Optuna best parameters\n",
        "num_units_1 = best_params['num_units_1']\n",
        "num_units_2 = best_params['num_units_2']\n",
        "num_units_3 = best_params['num_units_3']\n",
        "dropout_1 = best_params['dropout_1']\n",
        "dropout_2 = best_params['dropout_2']\n",
        "learning_rate = best_params['learning_rate']\n",
        "test_df['label'] = test_df['label'].astype(str)\n",
        "\n",
        "\n",
        "efficientnet_preds = np.load(\"/content/drive/My Drive/test_predictions_numtadb.npy\")\n",
        "\n",
        "\n",
        "\n",
        "cv_scores = []\n",
        "all_fold_preds = []\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_combined, y_train)):\n",
        "    print(f\"\\n===== Training Fold {fold+1} =====\")\n",
        "\n",
        "    # Split into training and validation fold\n",
        "    X_train_fold, X_val_fold = X_train_combined[train_idx], X_train_combined[val_idx]\n",
        "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    # Define MLP model\n",
        "    model = Sequential([\n",
        "        Dense(num_units_1, activation='selu', input_shape=(X_train_combined.shape[1],)),\n",
        "        BatchNormalization(), Dropout(dropout_1),\n",
        "\n",
        "        Dense(num_units_2, activation='selu'),\n",
        "        BatchNormalization(), Dropout(dropout_2),\n",
        "\n",
        "        Dense(num_units_3, activation='selu'),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train on current fold\n",
        "    model.fit(X_train_fold, y_train_fold, epochs=20, batch_size=64, verbose=0)\n",
        "\n",
        "    # Predict on test set for current fold\n",
        "    preds_proba = model.predict(X_test_combined)\n",
        "    all_fold_preds.append(preds_proba)\n",
        "\n",
        "# Average predicted probabilities across all folds\n",
        "avg_preds_proba = np.mean(np.array(all_fold_preds), axis=0)\n",
        "\n",
        "# Final predicted class labels from ensemble\n",
        "fusionnet_preds = np.argmax(avg_preds_proba, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_df['label'] = test_df['label'].astype(str)\n",
        "\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "import numpy as np\n",
        "\n",
        "# Ensure true labels are encoded consistently\n",
        "true_labels = encoder.transform(test_df['label'])  # Fixed line\n",
        "\n",
        "# Check alignment\n",
        "assert len(fusionnet_preds) == len(efficientnet_preds) == len(true_labels)\n",
        "\n",
        "# Determine correctness\n",
        "fusionnet_correct = fusionnet_preds == true_labels\n",
        "efficientnet_correct = efficientnet_preds == true_labels\n",
        "\n",
        "# Count mismatches\n",
        "B = np.sum((fusionnet_correct == True) & (efficientnet_correct == False))\n",
        "C = np.sum((fusionnet_correct == False) & (efficientnet_correct == True))\n",
        "\n",
        "# Contingency table\n",
        "table = [[0, B],\n",
        "         [C, 0]]\n",
        "\n",
        "# Run McNemar’s test\n",
        "result = mcnemar(table, exact=False, correction=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print result\n",
        "print(\"\\n========= McNemar’s Test Result =========\")\n",
        "print(f\"Contingency Table: [[0, {B}], [{C}, 0]]\")\n",
        "print(f\"McNemar's Test Statistic: {result.statistic:.4f}\")\n",
        "print(f\"P-value:  {result.pvalue:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcaLWj7SoBT2",
        "outputId": "01dd1431-dad4-4711-8ffc-aa21d19acdb0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= McNemar’s Test Result =========\n",
            "Contingency Table: [[0, 179], [218, 0]]\n",
            "McNemar's Test Statistic: 3.6372\n",
            "P-value:  0.0503\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}